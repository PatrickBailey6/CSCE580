{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d88506f5",
   "metadata": {},
   "source": [
    "### **Libraries used:**\n",
    "* Pandas\n",
    "* PyTorch\n",
    "* NumPy\n",
    "* RegExpressions\n",
    "* SciKitLearn\n",
    "* Transformers (HuggingFace)\n",
    "* Dataset (HuggingFace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a09b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CSV_PATH = \"IMDB Dataset.csv\"\n",
    "TEXT_COL = \"review\"\n",
    "LABEL_COL = \"sentiment\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import (\n",
    "    DistilBertForSequenceClassification,\n",
    "    DistilBertConfig,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "data_file = pd.read_csv(CSV_PATH)\n",
    "data_file.info()\n",
    "data_file.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fef6bc2",
   "metadata": {},
   "source": [
    "### **Preprocessing**\n",
    "The text is cleaned of any HTML syntax or embedded URLs for easy tokenization. Labels are then encoded for processing.\n",
    "Our test set will be split using the SKLearn train_test_split function, filtering 20% of the data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15b09afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewCleaner():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        text = str(text)\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d3b7b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = ReviewCleaner()\n",
    "data_file['clean_reviews'] = data_file[TEXT_COL].apply(cleaner.clean_text)\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "data_file['labels'] = label_encoder.fit_transform(data_file[LABEL_COL].tolist())\n",
    "\n",
    "train_data_file, test_data_file = train_test_split(data_file, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b2cc91",
   "metadata": {},
   "source": [
    "Pulling from Transformers, there are many pretrained tokenizers for DistilBERT. We'll use a basic lowercase tokenizer here, making 1 set for training and 1 set for testing created from the earlier split data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43f47b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a3b15a368f24487ac989418ece10fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15a3d95c8144d7ba2f3751d0c065597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_data_file)\n",
    "test_dataset = Dataset.from_pandas(test_data_file)\n",
    "\n",
    "def tokenize_data(i):\n",
    "    return tokenizer(i[\"clean_reviews\"], truncation=True)\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_data, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_data, batched=True)\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8cca95",
   "metadata": {},
   "source": [
    "### **Model Fine-Tuning**\n",
    "\n",
    "We will continue to use the Transformers library for easier automatic functions and algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7448761",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "NUM_LABELS = 2\n",
    "BATCH_SIZE = 16\n",
    "LR = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "EPOCHS = 3\n",
    "MAX_GRAD_NORM = 1.0\n",
    "WARMUP_STEPS = 0\n",
    "OUTPUT_DIR = \"./distilbert-finetuned\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_loader = DataLoader(tokenized_train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collator)\n",
    "test_loader = DataLoader(tokenized_test, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d084301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0053c9c1dcc8469a8697424ad58e7dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = DistilBertConfig.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "model.to(DEVICE)\n",
    "\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=LR)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50858c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    loss_total = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            loss_total += loss.item() * logits.size(0)\n",
    "            batch_preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            preds.extend(batch_preds.tolist())\n",
    "            labels.extend(batch[\"labels\"].cpu().numpy().tolist())\n",
    "    avg_loss = loss_total / len(dataloader.dataset)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\") if NUM_LABELS > 2 else f1_score(labels, preds, average=\"binary\")\n",
    "    return {\"loss\": avg_loss, \"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a497a036",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = evaluate(model, test_loader)\n",
    "print(f\"Epoch {EPOCHS+1}/{EPOCHS} test_loss={test_metrics['loss']:.4f} acc={test_metrics['accuracy']:.4f} f1={test_metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd71b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_f1 = -1.0\n",
    "global_step = 0\n",
    "total_steps = EPOCHS * len(train_loader)\n",
    "print(f\"Training on device {DEVICE}. Total steps = {total_steps}\")\n",
    "\n",
    "for EPOCHS in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        global_step += 1\n",
    "\n",
    "        if global_step % 100 == 0:\n",
    "            print(f\"Step {global_step}, loss {loss.item():.4f}\")\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    train_metrics = evaluate(model, train_loader)\n",
    "    print(f\"Epoch {EPOCHS+1}/{EPOCHS} train_loss={train_metrics['loss']:.4f} acc={train_metrics['accuracy']:.4f} f1={train_metrics['f1']:.4f}\")\n",
    "\n",
    "    if train_metrics[\"f1\"] > best_val_f1:\n",
    "        best_val_f1 = train_metrics[\"f1\"]\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        model.save_pretrained(OUTPUT_DIR)\n",
    "        tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "        print(f\"Saved best model to {OUTPUT_DIR} (f1={best_val_f1:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc718f70",
   "metadata": {},
   "source": [
    "GPT Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b15fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_MODEL_NAME = \"distilgpt2\"\n",
    "NUM_LABELS = 2\n",
    "OUTPUT_DIR = \"./gpt2-finetuned\"\n",
    "BATCH = 8\n",
    "LR = 2e-5\n",
    "EPOCHS = 3\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ad8695",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(GPT_MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(GPT_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cfc32c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
